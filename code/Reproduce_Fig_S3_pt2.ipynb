{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/numba/core/decorators.py:246: RuntimeWarning: nopython is set for njit and is ignored\n",
      "  warnings.warn('nopython is set for njit and is ignored', RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import anndata as ad\n",
    "import ast\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import geopandas as gpd\n",
    "import gseapy\n",
    "import hdf5plugin\n",
    "import importlib\n",
    "import itertools\n",
    "from math import sqrt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from numpy.polynomial import Polynomial\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import random\n",
    "import scanpy as sc\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import fisher_exact, ks_2samp\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "import skbio\n",
    "import slide_tcr_functions\n",
    "import squidpy as sq\n",
    "from statistics import mean, stdev\n",
    "import statsmodels.stats\n",
    "from statsmodels.stats import multitest\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import sys\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from collections import defaultdict\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "\n",
    "from alphashape import alphashape\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from libpysal.weights import KNN\n",
    "from matplotlib.collections import PatchCollection\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "from perm_test_functions import perm_test_to_final_plot\n",
    "from plotting_functions import gaussian_heat_plot, plot_cortex_medulla\n",
    "from shapely.geometry import Point\n",
    "from slide_tcr_functions import (\n",
    "    PuckReplicate,\n",
    "    save_puck,\n",
    "    load_puck,\n",
    "    make_puck,\n",
    "    make_adata,\n",
    "    load_adata,\n",
    "    pkl_load,\n",
    "    pkl_dump\n",
    ")\n",
    "from statsmodels import stats\n",
    "from tcr_mapping import tcr_mapping, get_barcode_position, build_6mer_dist, barcode_matching\n",
    "\n",
    "# Set directory paths\n",
    "directory = '../data/'  # Directory with all data\n",
    "output_directory = '../results/'\n",
    "\n",
    "# Set plotting parameters\n",
    "fontprops = fm.FontProperties(size=18)\n",
    "plt.rcParams[\"pdf.fonttype\"] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "pixels_needed = 500 / 0.65\n",
    "\n",
    "master_df = pd.read_csv(f'{directory}Sample_information.csv', header=0)\n",
    "\n",
    "# Initialize dictionaries and lists\n",
    "csv_name_to_puck_name = {}\n",
    "puck_name_to_timepoint = {}\n",
    "age_to_ss = {}\n",
    "puck_to_time = {}\n",
    "csv_names = []\n",
    "\n",
    "# Process rows in the master dataframe\n",
    "for index, row in master_df.iterrows():\n",
    "    puck_name = row['NovaSeq alignment directory']\n",
    "    age = row['Sample Age']\n",
    "\n",
    "    # Skip rows with specified Barcodes to TCR csv values\n",
    "    csv_name = f'{puck_name[11:25]}_B.csv'\n",
    "\n",
    "    # Populate dictionaries and lists\n",
    "    csv_name_to_puck_name[csv_name] = puck_name\n",
    "    csv_names.append(csv_name)\n",
    "    puck_name_to_timepoint[puck_name] = age\n",
    "\n",
    "    if age not in age_to_ss:\n",
    "        age_to_ss[age] = []\n",
    "    age_to_ss[age].append(row['NovaSeq alignment directory'])\n",
    "    puck_to_time[puck_name] = age\n",
    "\n",
    "# Define mappings and orderings\n",
    "time_to_puck = {puck_to_time[i]: i for i in puck_to_time}\n",
    "\n",
    "# Define timepoints, day mappings, and puck names\n",
    "\n",
    "tpt_to_pn = {\n",
    "    i: j\n",
    "    for i, j in zip(master_df['Sample Age'],\n",
    "                    master_df['NovaSeq alignment directory'])\n",
    "}\n",
    "tpt_order = [\n",
    "    '0 day', '1 day', '2 day', '3 day', '4 day', '5 day', '7 day', '10 day',\n",
    "    '13 day', '3wks', '4 wks', '5 wks', '7wks', '9 wks', '13 wks', '15wks',\n",
    "    '28wks', '32wks', '48 wks', '68 wks', '90wks'\n",
    "]\n",
    "\n",
    "timepoints = [\n",
    "    '1 day', '2 day', '3 day', '4 day', '5 day', '7 day', '10 day', '13 day',\n",
    "    '3wks', '4 wks', '5 wks', '7wks', '9 wks', '13 wks', '15wks', '28wks',\n",
    "    '32wks', '48 wks', '68 wks', '90wks'\n",
    "]\n",
    "tpt_to_day = {\n",
    "    '0 day': 0,\n",
    "    '1 day': 1,\n",
    "    '2 day': 2,\n",
    "    '3 day': 3,\n",
    "    '4 day': 4,\n",
    "    '5 day': 5,\n",
    "    '7 day': 7,\n",
    "    '10 day': 10,\n",
    "    '13 day': 13,\n",
    "    '3wks': 3 * 7,\n",
    "    '4 wks': 4 * 7,\n",
    "    '5 wks': 5 * 7,\n",
    "    '7wks': 7 * 7,\n",
    "    '9 wks': 9 * 7,\n",
    "    '13 wks': 13 * 7,\n",
    "    '15wks': 15 * 7,\n",
    "    '28wks': 28 * 7,\n",
    "    '32wks': 32 * 7,\n",
    "    '48 wks': 48 * 7,\n",
    "    '68 wks': 68 * 7,\n",
    "    '90wks': 90 * 7\n",
    "}\n",
    "\n",
    "puck_names = [\"2023-01-24_Puck_221024_23\", # 0 day\n",
    "                      \"2022-11-23_Puck_221024_24\", # 0 day\n",
    "                \"2022-11-23_Puck_221024_31\", # 1 day\n",
    "                      \"2023-01-16_Puck_221024_27\", # 1 day\n",
    "                      \"2022-11-23_Puck_221024_34\", # 2 day\n",
    "                      \"2023-01-24_Puck_221024_33\", # 2 day\n",
    "                      \"2023-01-24_Puck_221024_36\", # 2 day\n",
    "                      \"2022-11-23_Puck_221024_38\", # 3 day\n",
    "                      \"2023-01-24_Puck_221024_37\", # 3 day\n",
    "                      \"2023-01-24_Puck_221024_39\", # 3 day\n",
    "                      \"2023-01-24_Puck_221103_09\", # 4 day\n",
    "                      \"2023-01-24_Puck_221103_12\", # 4 day\n",
    "                      \"2023-01-24_Puck_221019_08\", # 5 day\n",
    "                      \"2023-01-24_Puck_221019_05\", # 5 day\n",
    "                      \"2023-01-24_Puck_221019_10\", # 7 day\n",
    "                      \"2023-01-24_Puck_221019_14\", # 7 day\n",
    "                      \"2022-12-16_Puck_221006_22_trunc\", # 10 day\n",
    "                      \"2023-01-24_Puck_221006_23\", # 10 day\n",
    "                      \"2022-12-16_Puck_221011_32_trunc\", # 13 day\n",
    "                      \"2022-12-16_Puck_221011_33_trunc\", # 13 day\n",
    "                      \"2023-01-24_Puck_221118_38\", # 3 weeks\n",
    "                      \"2023-01-24_Puck_221118_39\", # 3 weeks\n",
    "                      \"2023-01-24_Puck_221103_22\", # 4 weeks\n",
    "                      \"2023-01-24_Puck_221118_40\", # 4 weeks\n",
    "                      \"2023-01-24_Puck_220930_18\", # 5 weeks\n",
    "                      \"2023-01-24_Puck_220930_20\", # 5 weeks\n",
    "                      \"2023-01-24_Puck_220930_22\", # 5 weeks\n",
    "                      \"2023-01-24_Puck_220930_23\", # 7 weeks\n",
    "                      \"2023-01-24_Puck_220930_28\", # 7 weeks\n",
    "                      \"2023-01-24_Puck_221011_37\", # 9 weeks\n",
    "                      \"2023-01-24_Puck_221011_39\", # 9 weeks\n",
    "                      \"2023-01-24_Puck_221011_12\", # 13 weeks\n",
    "                      \"2023-01-24_Puck_221011_13\", # 13 weeks\n",
    "                      \"2022-11-04_Puck_221006_24\", # 15 week\n",
    "                      \"2022-12-16_Puck_221006_26_trunc\", # 15 week\n",
    "                      \"2022-11-04_Puck_221006_29\", # 28 week\n",
    "                      \"2022-12-16_Puck_221006_35_trunc\", # 28 week\n",
    "                      \"2022-11-04_Puck_220930_32\", # 32 week\n",
    "                      \"2022-12-16_Puck_220930_29_trunc\", # 32 week\n",
    "                      \"2023-01-24_Puck_220930_35\", # 32 week\n",
    "                      \"2023-01-24_Puck_221014_37\", # 48 week\n",
    "                      \"2023-01-24_Puck_221019_03\", # 48 week\n",
    "                      \"2022-12-16_Puck_221011_19_trunc\", # 68 week\n",
    "                      \"2022-12-16_Puck_221011_20_trunc\", # 68 week\n",
    "                      \"2023-01-24_Puck_221006_32\", # 68 week\n",
    "                      \"2022-12-16_Puck_220930_37_trunc\", # 90 week\n",
    "                      \"2022-12-16_Puck_220930_38_trunc\"] # 90 week\n",
    "\n",
    "puck_times = [0,0,1,1,2,2,2,3,3,3,4,4,5,5,7,7,10,10,13,13,3*7,3*7,4*7,4*7,5*7,5*7,5*7,\n",
    "              7*7,7*7,9*7,9*7,13*7,13*7,15*7,15*7,28*7,28*7,32*7,32*7,32*7,48*7,48*7,68*7,68*7,68*7,90*7,90*7]\n",
    "\n",
    "\n",
    "puck_name_to_days = dict(zip(puck_names, puck_times))\n",
    "\n",
    "# Define batch mappings\n",
    "batch_to_tpt = dict(zip(range(len(puck_times)), puck_times))\n",
    "batch_to_puck_name = dict(zip(range(len(puck_times)), puck_names))\n",
    "puck_to_batch = dict(zip(puck_names, range(len(puck_times))))\n",
    "\n",
    "def calculate_morans_i(x,y,label,label_of_interest):\n",
    "    # Create a GeoDataFrame\n",
    "    geometry = [Point(xy) for xy in zip(x, y)]\n",
    "    gdf = gpd.GeoDataFrame({'x': x, 'y': y, 'label': label}, geometry = geometry)\n",
    "\n",
    "    # Create spatial weights matrix using K-nearest neighbors\n",
    "    k = 5  # You can adjust the number of neighbors (k) as needed\n",
    "    w = KNN.from_dataframe(gdf, k=k)\n",
    "\n",
    "    # Calculate Moran's I for subset\n",
    "    moran_ct1 = Moran(gdf['label'].apply(lambda x: 1 if x == label_of_interest else 0), w)\n",
    "\n",
    "    # Print the Moran's I statistics\n",
    "    print(f\"Moran's I for {label_of_interest} Points:\")\n",
    "    print(\"I:\", moran_ct1.I)\n",
    "    print(\"Expected I:\", moran_ct1.EI)\n",
    "    print(\"p-value:\", moran_ct1.p_sim)\n",
    "\n",
    "    return moran_ct1.I\n",
    "\n",
    "def find_indices(lst, target_item):\n",
    "    return [index for index, item in enumerate(lst) if item == target_item]\n",
    "\n",
    "def dilate_cortex_points(bc_cortex, bc_medulla, bc_loc_dict_s1,\n",
    "                         loc_to_bc_s1, matched_bead_barcodes,\n",
    "                         loc_to_diversity, title, filt_plot=False,\n",
    "                         show_plot=True):\n",
    "    \"\"\"\n",
    "    Dilate cortex points based on the median diversity of nearby points.\n",
    "\n",
    "    Parameters:\n",
    "    - bc_cortex: List of cortex barcodes\n",
    "    - bc_medulla: List of medulla barcodes\n",
    "    - bc_loc_dict_s1: Dictionary mapping barcodes to locations\n",
    "    - loc_to_bc_s1: Dictionary mapping locations to barcodes\n",
    "    - matched_bead_barcodes: List of matched bead barcodes\n",
    "    - locs_cortex: List of cortex locations\n",
    "    - filt_plot: Boolean flag to filter the plot\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary mapping cortex locations to diluted diversity values\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract cortex locations\n",
    "    cortex_locs = [bc_loc_dict_s1[bc] for bc in bc_cortex]\n",
    "\n",
    "    # Extract all locations (cortex + medulla)\n",
    "    everywhere_locs = [bc_loc_dict_s1[bc] for bc in bc_medulla + bc_cortex]\n",
    "\n",
    "    # If there are fewer than 2 cortex locations, skip dilation\n",
    "    if len(cortex_locs) < 2:\n",
    "        return 'Skip'\n",
    "\n",
    "    point_tree = cKDTree(cortex_locs)\n",
    "\n",
    "    # Create a copy of loc_to_diversity for cortex locations\n",
    "    cort_loc_to_diversity = copy.deepcopy(loc_to_diversity)\n",
    "\n",
    "    # Iterative dilation process\n",
    "    num_iter = 0\n",
    "    while num_iter != 2:  # Limiting the number of iterations for demonstration (can be adjusted)\n",
    "        num_iter += 1\n",
    "        # print(f\"Iteration: {num_iter}\")\n",
    "        # print(f\"Number of Cortex Locations: {len(cort_loc_to_diversity)}\")\n",
    "\n",
    "        # Find unassigned cortex locations\n",
    "        unassigned_cortex_locs = [loc for loc in cortex_locs if loc not in cort_loc_to_diversity]\n",
    "\n",
    "        # Dilate unassigned cortex locations\n",
    "        for loc in unassigned_cortex_locs:\n",
    "            # Find points within a radius of 20 units\n",
    "            idx_closeby_points = point_tree.query_ball_point(loc, r=20 / 0.65)\n",
    "            nearest_points = [cortex_locs[idx] for idx in idx_closeby_points]\n",
    "\n",
    "            # Extract diversity values for nearby points\n",
    "            div = [cort_loc_to_diversity[i] for i in nearest_points if i in cort_loc_to_diversity]\n",
    "\n",
    "            # If there are diversity values, assign the median to the current location\n",
    "            if len(div) > 0:\n",
    "                cort_loc_to_diversity[loc] = np.median(div)\n",
    "\n",
    "        # Make a plot\n",
    "        if (show_plot & num_iter==2):\n",
    "            print('success')\n",
    "            plot_dilated_cortex(cort_loc_to_diversity, everywhere_locs, cortex_locs, title, filt_plot)\n",
    "    return cort_loc_to_diversity\n",
    "\n",
    "\n",
    "def plot_dilated_cortex(cort_loc_to_diversity, everywhere_locs, cortex_locs, title, filt_plot, vmin=3.3, vmax=5.4):\n",
    "    \"\"\"\n",
    "    Plot the dilated cortex.\n",
    "\n",
    "    Parameters:\n",
    "    - cort_loc_to_diversity: Dictionary mapping cortex locations to diluted diversity values\n",
    "    - everywhere_locs: List of all locations (cortex + medulla)\n",
    "    - cortex_locs: List of cortex locations\n",
    "    - locs_cortex: List of cortex locations for filtering\n",
    "    - filt_plot: Boolean flag to filter the plot\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    # Plot everywhere locations in gray\n",
    "    x2, y2 = zip(*everywhere_locs)\n",
    "    plt.scatter(x2, y2, c='gray', s=1, alpha=0.05)  # Everywhere\n",
    "\n",
    "    # Plot cortex locations in gray with higher alpha\n",
    "    x2, y2 = zip(*cortex_locs)\n",
    "    plt.scatter(x2, y2, c='gray', s=1, alpha=0.2)\n",
    "\n",
    "    # Plot dilated cortex locations\n",
    "    if filt_plot:\n",
    "        locs_filt, c = zip(*[(loc, c) for loc, c in zip(list(cort_loc_to_diversity.keys()), list(cort_loc_to_diversity.values())) if loc in cortex_locs])\n",
    "        x, y = zip(*locs_filt)\n",
    "    else:\n",
    "        x, y = zip(*list(cort_loc_to_diversity.keys()))\n",
    "        c = list(cort_loc_to_diversity.values())\n",
    "\n",
    "    # Scatter plot with color map\n",
    "    p = plt.scatter(x, y, c=c, s=1, cmap='inferno', vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # Additional plot settings\n",
    "    plt.axis('off')\n",
    "    plt.xlim([0, 5000])\n",
    "    plt.ylim([0, 5000])\n",
    "    plt.title(title)\n",
    "    cb_ax = fig.add_axes([.91, .124, .04, .754])\n",
    "    fig.colorbar(p, orientation='vertical', cax=cb_ax)\n",
    "    fontprops = fm.FontProperties(size=18)\n",
    "    scalebar = AnchoredSizeBar(\n",
    "        ax.transData,\n",
    "        pixels_needed,\n",
    "        \"\",\n",
    "        \"lower right\",\n",
    "        pad=0.1,\n",
    "        color=\"black\",\n",
    "        frameon=False,\n",
    "        size_vertical=1,\n",
    "        fontproperties=fontprops,\n",
    "    )\n",
    "    ax.add_artist(scalebar)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_gene_expression_on_pucks(adata_all, pucks_to_plot, genes_to_plot,title_name = None):\n",
    "    \"\"\"\n",
    "    Plot gene expression on specified pucks for selected genes.\n",
    "\n",
    "    Parameters:\n",
    "    - pucks_to_plot: List of puck names to be plotted\n",
    "    - genes_to_plot: List of gene names to be plotted\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    vmax_calc = {i: [] for i in genes_to_plot}\n",
    "    vmax_for_genes = {i: 0 for i in genes_to_plot}  # identify 90th percentile\n",
    "    vmin_calc = {i: [] for i in genes_to_plot}\n",
    "    vmin_for_genes = {i: 0 for i in genes_to_plot}  # identify 90th percentile\n",
    "    all_x_agg = {}\n",
    "    all_y_agg = {}\n",
    "    all_adata_filt = {}\n",
    "    all_total_counts = {}\n",
    "\n",
    "    # Calculate vmax values\n",
    "    for pn in tqdm(pucks_to_plot):\n",
    "        if pn not in loaded_pucks:\n",
    "            adata = adata_all[adata_all.obs.puck_name == pn]\n",
    "        else:\n",
    "            adata = loaded_pucks[pn]\n",
    "        all_x = adata.obs['x_val']\n",
    "        all_y = adata.obs['y_val']\n",
    "\n",
    "        all_x_agg[pn] = all_x\n",
    "        all_y_agg[pn] = all_y\n",
    "\n",
    "        adata = adata[adata.obs['cortex_medulla'].isin(['cortex', 'medulla'])]  # clean up areas outside tissue\n",
    "        all_adata_filt[pn] = adata\n",
    "\n",
    "        total_counts_per_cell = np.sum(adata.X, axis=1)\n",
    "        all_total_counts[pn] = total_counts_per_cell\n",
    "\n",
    "        for gene_name in genes_to_plot:\n",
    "            gene_idx = list(adata.var.index).index(gene_name)\n",
    "            gene_filt = adata.X[:, gene_idx]\n",
    "            gene_filt = [i / x * 10000 for i, x in zip(gene_filt, total_counts_per_cell)]\n",
    "\n",
    "            bc_indices = [index for index, val in enumerate(gene_filt) if val > 0]\n",
    "            gene_filt = [val for index, val in enumerate(gene_filt) if val > 0]\n",
    "\n",
    "            vmax_calc[gene_name] = vmax_calc[gene_name] + gene_filt\n",
    "            vmin_calc[gene_name] = vmin_calc[gene_name] + gene_filt\n",
    "\n",
    "    # Set percentile for plotting visualization\n",
    "    vmax_for_genes = {i: np.percentile(vmax_calc[i], 90) for i in vmax_calc}  # Make cutoff 90th percentile of max\n",
    "    vmin_for_genes = {i: np.percentile(vmin_calc[i], 10) for i in vmin_calc}  # Make cutoff 90th percentile of max\n",
    "\n",
    "    # Create plots\n",
    "    fig, ax = plt.subplots(figsize=(len(genes_to_plot) * 4, len(pucks_to_plot) * 4))\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt_cter = 1\n",
    "\n",
    "    # Make plots\n",
    "    for pn in tqdm(pucks_to_plot):\n",
    "        all_x = all_x_agg[pn]\n",
    "        all_y = all_y_agg[pn]\n",
    "\n",
    "        adata = all_adata_filt[pn]  # clean up areas outside tissue\n",
    "        total_counts_per_cell = all_total_counts[pn]\n",
    "\n",
    "        for gene_name in genes_to_plot:\n",
    "            ax = plt.subplot(len(pucks_to_plot), len(genes_to_plot), plt_cter)\n",
    "            ax.scatter(all_x, all_y, c='black', s=1)  # Plot locations in black\n",
    "\n",
    "            gene_idx = list(adata.var.index).index(gene_name)\n",
    "            gene_filt = adata.X[:, gene_idx]\n",
    "            gene_filt = [i / x * 10000 for i, x in zip(gene_filt, total_counts_per_cell)]\n",
    "            all_len = len(gene_filt)\n",
    "\n",
    "            bc_indices = [index for index, val in enumerate(gene_filt) if val > 0]\n",
    "            gene_filt = [val for index, val in enumerate(gene_filt) if val > 0]\n",
    "            x = [adata.obs['x_val'][i] for i in bc_indices]\n",
    "            y = [adata.obs['y_val'][i] for i in bc_indices]\n",
    "\n",
    "            vmax_calc[gene_name] = vmax_calc[gene_name] + gene_filt\n",
    "\n",
    "            vmin = vmin_for_genes[gene_name]\n",
    "            vmax = vmax_for_genes[gene_name]\n",
    "            thisfig = ax.scatter(x, y, c=gene_filt.toarray(), s=1, cmap='viridis', vmin=vmin, vmax=vmax)\n",
    "            fig.colorbar(thisfig)\n",
    "            ax.set_title(f\"{gene_name} {len(gene_filt)/all_len:.2%}\")\n",
    "\n",
    "            plt_cter += 1\n",
    "            ax.set_xlim([0, 5000])\n",
    "            ax.set_ylim([0, 5000])\n",
    "            ax.set_aspect('equal')\n",
    "\n",
    "            scalebar = AnchoredSizeBar(\n",
    "                ax.transData,\n",
    "                pixels_needed,\n",
    "                \"\",\n",
    "                \"lower right\",\n",
    "                pad=0.1,\n",
    "                color=\"black\",\n",
    "                frameon=False,\n",
    "                size_vertical=1,\n",
    "                fontproperties=fontprops,\n",
    "            )\n",
    "            ax.add_artist(scalebar)\n",
    "\n",
    "    plt.show()\n",
    "    plt.savefig(f'{output_directory}{title_name}.png', dpi=300)\n",
    "\n",
    "def rolling_average(x, y, title):\n",
    "    # Create a scatter plot of the data points\n",
    "    fig, ax = plt.subplots() \n",
    "    plt.scatter(x, y)\n",
    "    \n",
    "    # Calculate the slope (m) and intercept (b) of the line\n",
    "    m, b = np.polyfit(np.log10(x), y, 1)\n",
    "\n",
    "    # Generate x values for the line of best fit\n",
    "    x_fit = np.linspace(min(x), max(x), 10)\n",
    "\n",
    "    # Calculate corresponding y values using the equation of the line\n",
    "    y_fit = m * np.log10(x_fit) + b\n",
    "    \n",
    "    # Plot the line of best fit\n",
    "    plt.plot(x_fit, y_fit, color='red', label=f'Line of Best Fit: y = {m:.2f}x + {b:.2f}')\n",
    "\n",
    "    # Calculate and display Pearson correlation coefficient and p-value\n",
    "    stat, pval = scipy.stats.pearsonr(np.log10(timepoints), y)\n",
    "    plt.title(f'R = {round(stat,2)} | p = {round(pval,5)}')\n",
    "    \n",
    "    # Set x-axis scale to logarithmic\n",
    "    ax.set_xscale('log', base=10)\n",
    "    ax.set_xticks([20, 100, 500])\n",
    "    ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "    \n",
    "    # Save the plot as a PDF file\n",
    "    plt.savefig(f'{output_directory}S6D_{title}_sc_proportion.pdf')\n",
    "\n",
    "def find_nearest_point_distance(target_point, list_of_points):\n",
    "    \"\"\"\n",
    "    Find the point in a list of points closest to a target point.\n",
    "\n",
    "    Parameters:\n",
    "    - target_point (tuple): The target point in the form of (x, y).\n",
    "    - list_of_points (list of tuples): List of points in the form of [(x1, y1), (x2, y2), ...].\n",
    "\n",
    "    Returns:\n",
    "    - nearest_point (tuple): The point in the list closest to the target point.\n",
    "    - distance_to_nearest (float): The distance between the target point and the nearest point.\n",
    "    \"\"\"\n",
    "    target_point = np.array(target_point)\n",
    "    \n",
    "    # Calculate Euclidean distances between the target point and all points in the list\n",
    "    distances = distance.cdist(target_point, list_of_points, metric='euclidean')\n",
    "    \n",
    "    # Find the index of the nearest point\n",
    "    index_of_nearest = np.argmin(distances)\n",
    "\n",
    "    # Get the nearest point and its distance to the target point\n",
    "    nearest_point = list_of_points[index_of_nearest]\n",
    "    distance_to_nearest = distances[0, index_of_nearest]\n",
    "\n",
    "    return nearest_point, distance_to_nearest\n",
    "\n",
    "def calculate_distance_to_boundary(cluster_points, points_of_interest, alpha=0.03, show_plot = False):\n",
    "    \"\"\"\n",
    "    Calculate the distance from each point in a cluster to the boundary of the alpha shape.\n",
    "\n",
    "    Parameters:\n",
    "    - cluster_points (list of tuples): List of (x, y) coordinates for the points in the cluster.\n",
    "    - alpha (float): Alpha parameter for alpha shape. Smaller alpha values will create more detailed shapes.\n",
    "\n",
    "    Returns:\n",
    "    - distances (list of floats): List of distances from each point to the boundary.\n",
    "    - boundary (list of tuples): List of (x, y) coordinates representing the boundary of the alpha shape.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the list of tuples to a numpy array for compatibility with alphashape\n",
    "    points = np.array(cluster_points)\n",
    "    x,y = zip(*points)\n",
    "    # Create the alpha shape\n",
    "    alpha_shape = alphashape(points, alpha=alpha)\n",
    "\n",
    "    shape_x, shape_y = alpha_shape.exterior.coords.xy\n",
    "    \n",
    "    boundary = list(zip(shape_x,shape_y))\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=(3,3))\n",
    "        plt.plot(x, y, 'o', color='black', markersize=6)\n",
    "        plt.plot(shape_x, shape_y, 'o', color='red', markersize=4)\n",
    "        plt.xlim([0,5000])\n",
    "        plt.ylim([0,5000])\n",
    "        plt.show()\n",
    "    \n",
    "    # Calculate the distance from each point to the boundary\n",
    "    distances = [find_nearest_point_distance([point], boundary) for point in points_of_interest]\n",
    "\n",
    "    return alpha_shape, distances, boundary\n",
    "\n",
    "# Plot cell_types\n",
    "def plot_cell_types(adata, rctd_results, cell_type_list, title, title_save = None, colors=None, point_size=0.5):\n",
    "    \"\"\"\n",
    "    Plot cell types on a spatial map.\n",
    "\n",
    "    Parameters:\n",
    "    - adata: AnnData object containing spatial information.\n",
    "    - rctd_results: DataFrame containing RCTD results.\n",
    "    - cell_type_list: List of cell types to plot.\n",
    "    - title: Title of the plot.\n",
    "    - colors: List of colors for each cell type.\n",
    "    - point_size: Size of the plotted points.\n",
    "    \"\"\"\n",
    "    if colors is None:\n",
    "        colors = sns.color_palette('husl', len(cell_type_list))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    plt.xlim([0, 5000])\n",
    "    plt.ylim([0, 5000])\n",
    "\n",
    "    try:\n",
    "        loc_list = zip(adata.obs.x_val, adata.obs.y_val)\n",
    "    except:\n",
    "        loc_list = zip(adata.obs.x, adata.obs.y)\n",
    "    \n",
    "    adata.obs['barcode'] = [i.split('_')[2] for i in adata.obs.index]\n",
    "    bc_to_loc = {i: j for i, j in zip(adata.obs.barcode, loc_list)}\n",
    "\n",
    "    # Filter data for cortex and medulla regions\n",
    "    adata_filt = adata[adata.obs.cortex_medulla.isin(['cortex', 'medulla'])]\n",
    "    \n",
    "    try:\n",
    "        all_x, all_y = [adata_filt.obs.x_val, adata_filt.obs.y_val]\n",
    "    except:\n",
    "        all_x, all_y = [adata_filt.obs.x, adata_filt.obs.y]\n",
    "    plt.scatter(all_x, all_y, alpha=0.025, color='gray', s=1)\n",
    "\n",
    "    # Plot each cell type\n",
    "    for idx in range(len(cell_type_list)):\n",
    "        ct = cell_type_list[idx]\n",
    "        bcs = rctd_results[rctd_results['first_type'] == ct]['Unnamed: 0']\n",
    "        locs = [bc_to_loc[i] for i in bcs]\n",
    "        x, y = zip(*locs)\n",
    "        plt.scatter(x, y, s=point_size, c=colors[idx], label=ct)\n",
    "\n",
    "    # Add scale bar\n",
    "    scalebar = AnchoredSizeBar(\n",
    "        ax.transData,\n",
    "        pixels_needed,\n",
    "        \"\",\n",
    "        \"lower right\",\n",
    "        pad=0.1,\n",
    "        color=\"black\",\n",
    "        frameon=False,\n",
    "        size_vertical=1,\n",
    "        fontproperties=fontprops,\n",
    "    )\n",
    "    ax.add_artist(scalebar)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend() \n",
    "    if title_save!=None:\n",
    "        plt.savefig(f'{output_directory}{title_save}_{cell_type_list}_{title}.pdf')\n",
    "    plt.show()\n",
    "\n",
    "# Establish time point groupings\n",
    "def create_bins_dict(time_points,num_bins = 10, correct_timepoints = True): # default is deciles # correct timepoints makes sure the same time point is in the same decile\n",
    "    # Sort the time points in ascending order\n",
    "    sorted_time_points = sorted(time_points)\n",
    "    \n",
    "    # Calculate the indices for deciles\n",
    "    decile_indices = [int(len(sorted_time_points) * i / num_bins) for i in range(num_bins)]\n",
    "    \n",
    "    # Create the dictionary of deciles\n",
    "    deciles_dict = {}\n",
    "    \n",
    "    for i in range(num_bins):\n",
    "        decile_key = f'Decile {i+1}'\n",
    "        if i < num_bins-1:\n",
    "            decile_values = sorted_time_points[decile_indices[i]:decile_indices[i+1]]\n",
    "        else:\n",
    "            decile_values = sorted_time_points[decile_indices[i]:]\n",
    "        deciles_dict[decile_key] = decile_values\n",
    "    # Add the last decile separately\n",
    "    deciles_dict[f'Decile {num_bins}'] = sorted_time_points[decile_indices[-1]:]\n",
    "    \n",
    "    if correct_timepoints:\n",
    "        deciles_dict_corrected = {}\n",
    "        for i in range(num_bins):\n",
    "            decile_key = f'Decile {i+1}'\n",
    "            decile_key_old = f'Decile {i}'\n",
    "            if i == 0:\n",
    "                deciles_dict_corrected[decile_key] = deciles_dict[decile_key]\n",
    "                continue\n",
    "            else:\n",
    "                new = deciles_dict[decile_key]\n",
    "                old = deciles_dict_corrected[decile_key_old]\n",
    "                old_updated = old + [i for i in new if i in old] \n",
    "                new_updated =  [i for i in new if i not in old]\n",
    "                \n",
    "                deciles_dict_corrected[decile_key_old] = old_updated\n",
    "                deciles_dict_corrected[decile_key] = new_updated\n",
    "        deciles_dict = deciles_dict_corrected\n",
    "            \n",
    "            \n",
    "    return deciles_dict\n",
    "\n",
    "def subset_adata(adata, age, location):\n",
    "    \"\"\"\n",
    "    Filter the input AnnData object based on age and location.\n",
    "\n",
    "    Parameters:\n",
    "        adata (AnnData): The input AnnData object.\n",
    "        age (str): Age category ('young' or 'old').\n",
    "        location (str): Location category ('cortex' or 'medulla').\n",
    "\n",
    "    Returns:\n",
    "        adata_filt (AnnData): Filtered AnnData object based on age and location.\n",
    "    \"\"\"\n",
    "    # Filter AnnData based on age\n",
    "    adata_filt = adata[adata.obs['Age'].isin(age)]\n",
    "    # Further filter based on location\n",
    "    adata_filt = adata_filt[adata_filt.obs['cortex_medulla'] == location]\n",
    "    return adata_filt\n",
    "\n",
    "def young_old_props(adata):\n",
    "    \"\"\"\n",
    "    Calculate cell type proportions for young and old samples.\n",
    "\n",
    "    Parameters:\n",
    "        adata (AnnData): The input AnnData object.\n",
    "\n",
    "    Returns:\n",
    "        young (list): List of dictionaries containing cell type proportions for young samples.\n",
    "        old (list): List of dictionaries containing cell type proportions for old samples.\n",
    "    \"\"\"\n",
    "    young = []\n",
    "    old = []\n",
    "    for age in tqdm(set(adata.obs['Age'])):\n",
    "        # Subset AnnData for the specific age\n",
    "        adata_filt_age = adata[adata.obs['Age'] == age]\n",
    "        # Calculate cell type proportions\n",
    "        cter = Counter(adata_filt_age.obs['Cell_type'])\n",
    "        cter_norm = {i: cter[i] / sum(cter.values()) for i in cter}\n",
    "        # Append proportions to young or old based on age category\n",
    "        if age in young_ages:\n",
    "            young.append(cter_norm)\n",
    "        elif age in old_ages:\n",
    "            old.append(cter_norm)\n",
    "        else:\n",
    "            print('Unexpected age category')\n",
    "    return young, old\n",
    "\n",
    "def cohens_d(c0, c1):\n",
    "    \"\"\"\n",
    "    Calculate Cohen's d effect size.\n",
    "\n",
    "    Parameters:\n",
    "        c0 (list): Values for group 0.\n",
    "        c1 (list): Values for group 1.\n",
    "\n",
    "    Returns:\n",
    "        cohens_d (float): Cohen's d effect size.\n",
    "    \"\"\"\n",
    "    cohens_d = (mean(c0) - mean(c1)) / (sqrt((stdev(c0) ** 2 + stdev(c1) ** 2) / 2))\n",
    "    return cohens_d\n",
    "\n",
    "def find_cohens_d_by_ct(adata, young, old):\n",
    "    \"\"\"\n",
    "    Find Cohen's d effect size by cell type.\n",
    "\n",
    "    Parameters:\n",
    "        adata (AnnData): The input AnnData object.\n",
    "        young (list): List of dictionaries containing cell type proportions for young samples.\n",
    "        old (list): List of dictionaries containing cell type proportions for old samples.\n",
    "\n",
    "    Returns:\n",
    "        ct (list): List of cell types sorted by Cohen's d effect size.\n",
    "        vals (list): List of Cohen's d effect sizes sorted by magnitude.\n",
    "        pvals (dict): Dictionary of p-values for t-tests between young and old samples for each cell type.\n",
    "    \"\"\"\n",
    "    cohens_d_by_cell_type = {}\n",
    "    pvals = {}\n",
    "    for cell_type in tqdm(set(adata_filt.obs['Cell_type'])):\n",
    "        # Get proportions for young and old samples for the current cell type\n",
    "        young_cell_type_prop = [i[cell_type] for i in young if cell_type in i]\n",
    "        old_cell_type_prop = [i[cell_type] for i in old if cell_type in i]\n",
    "\n",
    "        # Calculate Cohen's d effect size\n",
    "        cd = cohens_d(old_cell_type_prop, young_cell_type_prop)\n",
    "        cohens_d_by_cell_type[cell_type] = cd\n",
    "\n",
    "        # Perform t-test and store p-value\n",
    "        stat, pval = scipy.stats.ttest_ind(young_cell_type_prop, old_cell_type_prop)\n",
    "        pvals[cell_type] = pval\n",
    "\n",
    "    # Sort cell types by Cohen's d effect size\n",
    "    sorted_cohens_d_by_cell_type = sorted(cohens_d_by_cell_type.items(), key=lambda x: x[1], reverse=True)\n",
    "    ct = [i[0] for i in sorted_cohens_d_by_cell_type]\n",
    "    vals = [i[1] for i in sorted_cohens_d_by_cell_type]\n",
    "    return ct, vals, pvals\n",
    "\n",
    "with open(f'{directory}Thymus_data_TCR_locs_diver.h5',\"rb\") as handle:\n",
    "    adata_diver = sc.read_h5ad(handle) \n",
    "\n",
    "with open(f'{directory}thymuscombined_xy.h5ad', \"rb\") as handle:\n",
    "    adata_combined = sc.read_h5ad(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Figure S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = copy.deepcopy(adata_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_type = 'medulla'\n",
    "\n",
    "# Initialize dictionaries to store data\n",
    "all_cell_type_pairs = {}  # Dictionary to store z-scores for cell type pairs\n",
    "arr_all = {}  # Dictionary to store z-score DataFrames for each timepoint\n",
    "\n",
    "# Iterate over each puck name and load corresponding data\n",
    "for puck_name in tqdm(puck_names):\n",
    "    # Skip specific pucks with insufficient cells in the medulla region\n",
    "    if region_type == 'medulla' and puck_name in {\n",
    "        '2022-11-23_Puck_221024_34',  # Not enough cells, 2 days\n",
    "        '2023-01-24_Puck_221103_09',  # Not enough cells, 4 days\n",
    "        '2023-01-24_Puck_221103_12',  # Not enough cells, 4 days\n",
    "    }:\n",
    "        continue\n",
    "    \n",
    "    # Define the file path for loading the data\n",
    "    file_path = f'{directory}permutation_testing/zscore_{puck_name}_pd_{region_type}.pkl'\n",
    "    \n",
    "    try:\n",
    "        # Load the data from the file\n",
    "        with open(file_path, 'rb') as f:\n",
    "            z_score_df = pk.load(f)\n",
    "    except FileNotFoundError:\n",
    "        # Handle missing data files gracefully\n",
    "        print(f'File missing: {puck_name}')\n",
    "        continue\n",
    "\n",
    "# Process and store data for each puck\n",
    "for puck_name in puck_names:\n",
    "    # Get the corresponding timepoint\n",
    "    timepoint = puck_to_time[puck_name]\n",
    "\n",
    "    # Define the file path\n",
    "    file_path = f'{directory}permutation_testing/zscore_{puck_name}_pd_{region_type}.pkl'\n",
    "\n",
    "    try:\n",
    "        # Load data from the file\n",
    "        with open(file_path, 'rb') as f:\n",
    "            df = pk.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f'{timepoint} {puck_name} is missing')\n",
    "        continue\n",
    "\n",
    "    # Store the loaded DataFrame in the dictionary\n",
    "    arr_all[timepoint] = df\n",
    "\n",
    "    # Extract cell type labels from DataFrame\n",
    "    cell_types_col = df.columns  # Column cell types\n",
    "    cell_types_row = df.index    # Row cell types\n",
    "    z_scores = np.array(df)  # Convert DataFrame to a NumPy array for easier indexing\n",
    "\n",
    "    # Iterate over each cell type pair\n",
    "    for i, c1 in enumerate(cell_types_col):\n",
    "        for j, c2 in enumerate(cell_types_row):\n",
    "            # Store the z-score for this cell type pair at the given timepoint\n",
    "            if (c1, c2) not in all_cell_type_pairs:\n",
    "                all_cell_type_pairs[(c1, c2)] = []\n",
    "            all_cell_type_pairs[(c1, c2)].append((timepoint, z_scores[i, j]))  # Append (timepoint, z-score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store line plots for each cell type pair\n",
    "line_plots = {}\n",
    "\n",
    "# Iterate over all cell type pairs\n",
    "for i in all_cell_type_pairs:\n",
    "    num_pairs = len(all_cell_type_pairs[i])\n",
    "    line_plots[i] = all_cell_type_pairs[i]\n",
    "\n",
    "# Boolean indicating whether to correct p-values\n",
    "corr_pval = True\n",
    "\n",
    "# Initial color for plotting\n",
    "color = 'gray'\n",
    "\n",
    "# Initialize a list to store p-values for correction\n",
    "pval_recorder_for_corr = []\n",
    "\n",
    "# Counter for the subplots\n",
    "plt_cter = 0\n",
    "\n",
    "# Iterate over each cell type pair for plotting\n",
    "for i in line_plots:\n",
    "    if len(line_plots[i]) <= 1:\n",
    "        continue\n",
    "    # Filter out NaN values from the line plots\n",
    "    line_plots_filt = [j for j in line_plots[i] if ~np.isnan(j[1])]\n",
    "    y_vals = [j[1] for j in line_plots_filt]\n",
    "    x_vals = [tpt_to_day[j[0]] for j in line_plots_filt]\n",
    "    x_vals = [i+1 for i in x_vals]\n",
    "    # Sort x and y values\n",
    "    x_vals, y_vals = zip(*sorted([(i,j) for i,j in zip(x_vals,y_vals)]))\n",
    "    x_vals, y_vals = zip(*sorted([(i,j) for i,j in zip(x_vals,y_vals) if np.isfinite(j)]))\n",
    "    \n",
    "    if len(x_vals) < 5:\n",
    "        continue\n",
    "    # Calculate Pearson correlation coefficient and p-value\n",
    "    corr, pval = scipy.stats.pearsonr(np.log10(x_vals), y_vals)\n",
    "\n",
    "    # Store p-value for correction\n",
    "    pval_recorder_for_corr.append(pval)\n",
    "\n",
    "# Perform multiple testing correction on p-values\n",
    "corr_list = stats.multitest.multipletests(pval_recorder_for_corr, method='fdr_bh')\n",
    "corr_dict = {i: j for i, j in zip(pval_recorder_for_corr, corr_list[1])}\n",
    "\n",
    "# For plotting\n",
    "dotplot_sig_interactions = []\n",
    "dotplot_sig_corr = []\n",
    "dotplot_sig_pval = []\n",
    "\n",
    "# Create subplots for individual interactions\n",
    "\n",
    "if region_type == 'cortex':\n",
    "    fig, axs = plt.subplots(nrows=35, ncols=5, figsize=(16, 7*12))\n",
    "elif region_type == 'medulla':\n",
    "    fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(16, 12))\n",
    "    \n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# Reset the counter for subplots\n",
    "plt_cter = 0\n",
    "\n",
    "# Iterate over each cell type pair for plotting\n",
    "for i in tqdm(line_plots):\n",
    "    if len(line_plots[i]) <= 1:\n",
    "        continue\n",
    "    line_plots_filt = [j for j in line_plots[i] if ~np.isnan(j[1])]\n",
    "    y_vals = [j[1] for j in line_plots[i]]\n",
    "    x_vals = [tpt_to_day[j[0]] for j in line_plots[i]]\n",
    "    x_vals = [i+1 for i in x_vals]\n",
    "    x_vals, y_vals = zip(*sorted([(i,j) for i,j in zip(x_vals,y_vals) if np.isfinite(j)]))\n",
    "    if len(x_vals) < 5:\n",
    "        continue\n",
    "    # Calculate Pearson correlation coefficient and p-value\n",
    "    corr, pval = scipy.stats.pearsonr(np.log10(x_vals), y_vals)\n",
    "    if corr_pval:\n",
    "        # Correct p-value if needed\n",
    "        pval = corr_dict[pval]\n",
    "    dotplot_sig_interactions.append(i)\n",
    "    dotplot_sig_corr.append(corr)\n",
    "    dotplot_sig_pval.append(pval)\n",
    "    \n",
    "    # Plot significant interactions\n",
    "    if (pval < 0.05):\n",
    "        y_vals_df = pd.DataFrame.from_dict({'y': y_vals})\n",
    "        y_vals_line = y_vals_df['y'].rolling(window=5, min_periods=1, center=True).mean()\n",
    "        alpha_val = 1\n",
    "        print(f'{i[0]} and {i[1]}')\n",
    "        ax_val = axs.ravel()[plt_cter]\n",
    "        plt_cter += 1\n",
    "        ax_val.scatter(x_vals, y_vals, c=color, alpha=alpha_val, s=5)\n",
    "        ax_val.plot(x_vals, y_vals_line, c='black', alpha=alpha_val)\n",
    "        \n",
    "        ax_val.set_title(str(i)+' \\n pval: '+str(round(pval, 8)) + ' corr: '+str(round(corr, 2)))\n",
    "        ax_val.set_xscale('log', base=10)\n",
    "        ax_val.set_xticks([1, 5, 10, 100, 500])\n",
    "        ax_val.get_xaxis().set_major_formatter(ScalarFormatter())\n",
    "        \n",
    "    else:\n",
    "        color = 'gray'\n",
    "        alpha_val = 0.01\n",
    "        continue\n",
    "        \n",
    "# Set the label for x-axis\n",
    "plt.xlabel('timepoints, log+1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique cell types from dotplot_sig_interactions\n",
    "all_ct = list(set([i[0] for i in dotplot_sig_interactions] + [i[1][:-2] for i in dotplot_sig_interactions]))\n",
    "\n",
    "# Remove specific cell types based on region type\n",
    "if region_type == 'medulla':\n",
    "    all_ct = [i for i in all_ct if 'cTEC' not in i]\n",
    "elif region_type == 'cortex':\n",
    "    all_ct = [i for i in all_ct if 'mTEC' not in i]\n",
    "\n",
    "# Initialize matrices for heatmap\n",
    "heatmap_mat = np.zeros([len(all_ct),len(all_ct)])\n",
    "heatmap_pval = np.zeros([len(all_ct),len(all_ct)])\n",
    "\n",
    "# Loop over each interaction\n",
    "for idx in range(len(dotplot_sig_interactions)):\n",
    "    sig_int = dotplot_sig_interactions[idx]\n",
    "    corr = dotplot_sig_corr[idx]\n",
    "    pval = dotplot_sig_pval[idx]\n",
    "\n",
    "    # Skip interactions involving specific cell types based on region type\n",
    "    ct1 = sig_int[0]\n",
    "    ct2 = sig_int[1]\n",
    "    if region_type == 'medulla':\n",
    "        if 'cTEC' in ct1 or 'cTEC' in ct2:\n",
    "            continue\n",
    "    elif region_type == 'cortex':\n",
    "        if 'mTEC' in ct1 or 'mTEC' in ct2:\n",
    "            continue\n",
    "    \n",
    "    # Get indices of cell types in the heatmap matrices\n",
    "    ct1_idx = all_ct.index(ct1)\n",
    "    ct2_idx = all_ct.index(ct2[:-2])\n",
    "    \n",
    "    # Populate heatmap matrices\n",
    "    heatmap_mat[ct1_idx][ct2_idx] = corr\n",
    "    heatmap_pval[ct1_idx][ct2_idx] = pval\n",
    "\n",
    "# Create DataFrame for heatmap data and remove rows/columns with all zeros\n",
    "heatmap_df = pd.DataFrame(heatmap_mat,index=all_ct,columns=all_ct)\n",
    "heatmap_df = heatmap_df.loc[~(heatmap_df==0).all(axis=1)]\n",
    "heatmap_df = heatmap_df.loc[:, (heatmap_df != 0).any(axis=0)]\n",
    "\n",
    "# Create DataFrame for p-values and remove rows/columns with all zeros\n",
    "heatmap_pv = pd.DataFrame(heatmap_pval,index=all_ct,columns=all_ct)\n",
    "heatmap_pv = heatmap_pv.loc[~(heatmap_pv==0).all(axis=1)]\n",
    "heatmap_pv = heatmap_pv.loc[:, (heatmap_pv != 0).any(axis=0)]\n",
    "\n",
    "# Create clustermap to reorder the data frame\n",
    "cm = sns.color_palette(\"vlag\", as_cmap=True)\n",
    "fig = sns.clustermap(heatmap_df,cmap=cm,xticklabels=True,yticklabels=True,\n",
    "                     figsize=(15,10),row_cluster=True,col_cluster=True)\n",
    "fig.savefig('cortex_interaction_heatmap.pdf')\n",
    "\n",
    "# Reorder data frame based on clustermap\n",
    "ind = fig.dendrogram_col.reordered_ind\n",
    "set_order = [heatmap_df.index[i] for i in ind]\n",
    "if region_type == 'medulla':\n",
    "    set_order = ['DC2', 'Mac', 'B', 'CD4+T', 'CD8+T', 'Ery', 'Endo', 'Fb', 'DC1', 'Treg', 'alpha_beta_T(entry)', 'pDC', 'Epi_unknown', 'VSMC', 'NK', 'IELpA', 'HSC', 'TEC_early', 'IELpB_NKT', 'Mono', 'DN(Q)', 'DP(Q)', 'mTEC', 'gdT', 'DN(P)', 'DP(P)']\n",
    "elif region_type == 'cortex':\n",
    "    set_order = ['DC2', 'Mac', 'B', 'CD4+T', 'CD8+T', 'Ery', 'Endo', 'Fb', 'DC1', 'Treg', 'alpha_beta_T(entry)', 'pDC', 'Epi_unknown', 'VSMC', 'NK', 'IELpA', 'HSC', 'TEC_early', 'IELpB_NKT', 'Mono', 'DN(Q)', 'DP(Q)', 'cTEC', 'gdT', 'DN(P)', 'DP(P)']\n",
    "heatmap_df = heatmap_df[set_order]\n",
    "heatmap_df = heatmap_df.reindex(index = set_order)\n",
    "\n",
    "heatmap_pv = heatmap_pv[set_order]\n",
    "heatmap_pv = heatmap_pv.reindex(index = set_order)\n",
    "\n",
    "# Create scattered heat map\n",
    "M = 26\n",
    "N = 26\n",
    "xlabels = heatmap_df.columns\n",
    "ylabels = heatmap_df.index\n",
    "\n",
    "x,y = np.meshgrid(np.arange(M), np.arange(N))\n",
    "s = -np.log10(heatmap_pv.values) # Size\n",
    "c = heatmap_df.values # Color\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "R = (s+2)/(s.max()+2)/2\n",
    "circles = [plt.Circle((j,i), radius=r) for r, j, i in zip(R.flat, x.flat, y.flat)]\n",
    "col = PatchCollection(circles, array=c.flatten(), cmap=\"vlag\")\n",
    "ax.add_collection(col)\n",
    "\n",
    "ax.set(xticks=np.arange(M), yticks=np.arange(N),\n",
    "       xticklabels=xlabels, yticklabels=ylabels)\n",
    "ax.set_xticks(np.arange(M+1)-0.5, minor=True)\n",
    "ax.set_yticks(np.arange(N+1)-0.5, minor=True)\n",
    "ax.grid(which='minor')\n",
    "plt.xticks(rotation = 90)\n",
    "\n",
    "plt.savefig(f'fig2b_cb_{region_type}.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Create scattered heat map with colorbar\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "R = (s+2)/(s.max()+2)/2\n",
    "circles = [plt.Circle((j,i), radius=r) for r, j, i in zip(R.flat, x.flat, y.flat)]\n",
    "col = PatchCollection(circles, array=c.flatten(), cmap=\"vlag\")\n",
    "ax.add_collection(col)\n",
    "\n",
    "ax.set(xticks=np.arange(M), yticks=np.arange(N),\n",
    "       xticklabels=xlabels, yticklabels=ylabels)\n",
    "ax.set_xticks(np.arange(M+1)-0.5, minor=True)\n",
    "ax.set_yticks(np.arange(N+1)-0.5, minor=True)\n",
    "ax.grid(which='minor')\n",
    "plt.xticks(rotation = 90)\n",
    "\n",
    "fig.colorbar(col)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(f'{output_directory}S3D.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = copy.deepcopy(adata_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "region_types = ['cortex','medulla']\n",
    "\n",
    "for region_type in region_types:\n",
    "    print(region_type)\n",
    "    print()\n",
    "    # Initialize dictionaries to store data\n",
    "    all_cell_type_pairs = {}  # Dictionary to store z-scores for cell type pairs\n",
    "    arr_all = {}  # Dictionary to store z-score DataFrames for each timepoint\n",
    "    \n",
    "    # Iterate over each puck name and load corresponding data\n",
    "    for puck_name in tqdm(puck_names):\n",
    "        # Skip specific pucks with insufficient cells in the medulla region\n",
    "        if region_type == 'medulla' and puck_name in {\n",
    "            '2022-11-23_Puck_221024_34',  # Not enough cells, 2 days\n",
    "            '2023-01-24_Puck_221103_09',  # Not enough cells, 4 days\n",
    "            '2023-01-24_Puck_221103_12',  # Not enough cells, 4 days\n",
    "        }:\n",
    "            continue\n",
    "        \n",
    "        # Define the file path for loading the data\n",
    "        file_path = f'{directory}permutation_testing/zscore_{puck_name}_pd_{region_type}.pkl'\n",
    "        \n",
    "        try:\n",
    "            # Load the data from the file\n",
    "            with open(file_path, 'rb') as f:\n",
    "                z_score_df = pk.load(f)\n",
    "        except FileNotFoundError:\n",
    "            # Handle missing data files gracefully\n",
    "            print(f'File missing: {puck_name}')\n",
    "            continue\n",
    "    \n",
    "    # Process and store data for each puck\n",
    "    for puck_name in puck_names:\n",
    "        # Get the corresponding timepoint\n",
    "        timepoint = puck_to_time[puck_name]\n",
    "    \n",
    "        # Define the file path\n",
    "        file_path = f'{directory}permutation_testing/zscore_{puck_name}_pd_{region_type}.pkl'\n",
    "    \n",
    "        try:\n",
    "            # Load data from the file\n",
    "            with open(file_path, 'rb') as f:\n",
    "                df = pk.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f'{timepoint} {puck_name} is missing')\n",
    "            continue\n",
    "    \n",
    "        # Store the loaded DataFrame in the dictionary\n",
    "        arr_all[timepoint] = df\n",
    "    \n",
    "        # Extract cell type labels from DataFrame\n",
    "        cell_types_col = df.columns  # Column cell types\n",
    "        cell_types_row = df.index    # Row cell types\n",
    "        z_scores = np.array(df)  # Convert DataFrame to a NumPy array for easier indexing\n",
    "    \n",
    "        # Iterate over each cell type pair\n",
    "        for i, c1 in enumerate(cell_types_col):\n",
    "            for j, c2 in enumerate(cell_types_row):\n",
    "                # Store the z-score for this cell type pair at the given timepoint\n",
    "                if (c1, c2) not in all_cell_type_pairs:\n",
    "                    all_cell_type_pairs[(c1, c2)] = []\n",
    "                all_cell_type_pairs[(c1, c2)].append((timepoint, z_scores[i, j]))  # Append (timepoint, z-score)\n",
    "    # Initialize an empty dictionary to store line plots for each cell type pair\n",
    "    line_plots = {}\n",
    "    \n",
    "    # Iterate over all cell type pairs\n",
    "    for i in all_cell_type_pairs:\n",
    "        num_pairs = len(all_cell_type_pairs[i])\n",
    "        line_plots[i] = all_cell_type_pairs[i]\n",
    "    \n",
    "    # Boolean indicating whether to correct p-values\n",
    "    corr_pval = True\n",
    "    \n",
    "    # Initial color for plotting\n",
    "    color = 'gray'\n",
    "    \n",
    "    # Initialize a list to store p-values for correction\n",
    "    pval_recorder_for_corr = []\n",
    "    \n",
    "    # Counter for the subplots\n",
    "    plt_cter = 0\n",
    "    \n",
    "    # Iterate over each cell type pair for plotting\n",
    "    for i in line_plots:\n",
    "        if len(line_plots[i]) <= 1:\n",
    "            continue\n",
    "        # Filter out NaN values from the line plots\n",
    "        line_plots_filt = [j for j in line_plots[i] if ~np.isnan(j[1])]\n",
    "        y_vals = [j[1] for j in line_plots_filt]\n",
    "        x_vals = [tpt_to_day[j[0]] for j in line_plots_filt]\n",
    "        x_vals = [i+1 for i in x_vals]\n",
    "        # Sort x and y values\n",
    "        x_vals, y_vals = zip(*sorted([(i,j) for i,j in zip(x_vals,y_vals)]))\n",
    "        x_vals, y_vals = zip(*sorted([(i,j) for i,j in zip(x_vals,y_vals) if np.isfinite(j)]))\n",
    "        \n",
    "        if len(x_vals) < 5:\n",
    "            continue\n",
    "        # Calculate Pearson correlation coefficient and p-value\n",
    "        corr, pval = scipy.stats.pearsonr(np.log10(x_vals), y_vals)\n",
    "    \n",
    "        # Store p-value for correction\n",
    "        pval_recorder_for_corr.append(pval)\n",
    "    \n",
    "    # Perform multiple testing correction on p-values\n",
    "    corr_list = stats.multitest.multipletests(pval_recorder_for_corr, method='fdr_bh')\n",
    "    corr_dict = {i: j for i, j in zip(pval_recorder_for_corr, corr_list[1])}\n",
    "    \n",
    "    # For plotting\n",
    "    dotplot_sig_interactions = []\n",
    "    dotplot_sig_corr = []\n",
    "    dotplot_sig_pval = []\n",
    "    \n",
    "    # Create subplots for individual interactions\n",
    "    \n",
    "    if region_type == 'cortex':\n",
    "        fig, axs = plt.subplots(nrows=35, ncols=5, figsize=(16, 7*12))\n",
    "    elif region_type == 'medulla':\n",
    "        fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(16, 12))\n",
    "        \n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    \n",
    "    # Reset the counter for subplots\n",
    "    plt_cter = 0\n",
    "    \n",
    "    # Iterate over each cell type pair for plotting\n",
    "    for i in tqdm(line_plots):\n",
    "        if len(line_plots[i]) <= 1:\n",
    "            continue\n",
    "        line_plots_filt = [j for j in line_plots[i] if ~np.isnan(j[1])]\n",
    "        y_vals = [j[1] for j in line_plots[i]]\n",
    "        x_vals = [tpt_to_day[j[0]] for j in line_plots[i]]\n",
    "        x_vals = [i+1 for i in x_vals]\n",
    "        x_vals, y_vals = zip(*sorted([(i,j) for i,j in zip(x_vals,y_vals) if np.isfinite(j)]))\n",
    "        if len(x_vals) < 5:\n",
    "            continue\n",
    "        # Calculate Pearson correlation coefficient and p-value\n",
    "        corr, pval = scipy.stats.pearsonr(np.log10(x_vals), y_vals)\n",
    "        if corr_pval:\n",
    "            # Correct p-value if needed\n",
    "            pval = corr_dict[pval]\n",
    "        dotplot_sig_interactions.append(i)\n",
    "        dotplot_sig_corr.append(corr)\n",
    "        dotplot_sig_pval.append(pval)\n",
    "        \n",
    "        # Plot significant interactions\n",
    "        if (pval < 0.05):\n",
    "            y_vals_df = pd.DataFrame.from_dict({'y': y_vals})\n",
    "            y_vals_line = y_vals_df['y'].rolling(window=5, min_periods=1, center=True).mean()\n",
    "            alpha_val = 1\n",
    "            print(f'{i[0]} and {i[1]}')\n",
    "            ax_val = axs.ravel()[plt_cter]\n",
    "            plt_cter += 1\n",
    "            ax_val.scatter(x_vals, y_vals, c=color, alpha=alpha_val, s=5)\n",
    "            ax_val.plot(x_vals, y_vals_line, c='black', alpha=alpha_val)\n",
    "            \n",
    "            ax_val.set_title(str(i)+' \\n pval: '+str(round(pval, 8)) + ' corr: '+str(round(corr, 2)))\n",
    "            ax_val.set_xscale('log', base=10)\n",
    "            ax_val.set_xticks([1, 5, 10, 100, 500])\n",
    "            ax_val.get_xaxis().set_major_formatter(ScalarFormatter())\n",
    "            \n",
    "        else:\n",
    "            color = 'gray'\n",
    "            alpha_val = 0.01\n",
    "            continue\n",
    "            \n",
    "    # Set the label for x-axis\n",
    "    plt.xlabel('timepoints, log+1')\n",
    "    plt.savefig(f'{output_directory}S3E_{region_type}.pdf')\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
